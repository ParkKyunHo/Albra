#!/usr/bin/env python3
"""
ì˜¤ë¥˜ ì¶”ì  ë° ë¶„ì„ ë„êµ¬
ë¡œê·¸ íŒŒì¼ì—ì„œ ì˜¤ë¥˜ë¥¼ ìˆ˜ì§‘í•˜ê³  íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import os
import re
import json
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Tuple, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent
CLAUDE_DIR = PROJECT_ROOT / '.claude'
LOGS_DIR = PROJECT_ROOT / 'logs'

class ErrorTracker:
    def __init__(self):
        self.errors = []
        self.patterns = defaultdict(int)
        
    def scan_log_files(self, days: int = 7) -> List[Dict]:
        """ë¡œê·¸ íŒŒì¼ì—ì„œ ì˜¤ë¥˜ ìŠ¤ìº”"""
        errors = []
        since_date = datetime.now() - timedelta(days=days)
        
        # ë¡œê·¸ íŒŒì¼ íŒ¨í„´
        log_patterns = ['*.log', '*_error.log']
        
        for pattern in log_patterns:
            for log_file in LOGS_DIR.glob(pattern):
                if log_file.stat().st_mtime < since_date.timestamp():
                    continue
                    
                try:
                    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                        for line_num, line in enumerate(f, 1):
                            error_info = self._parse_error_line(line, log_file, line_num)
                            if error_info:
                                errors.append(error_info)
                except Exception as e:
                    print(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {log_file}: {e}")
        
        return errors
    
    def _parse_error_line(self, line: str, file_path: Path, line_num: int) -> Optional[Dict]:
        """ì˜¤ë¥˜ ë¼ì¸ íŒŒì‹±"""
        # ì˜¤ë¥˜ íŒ¨í„´ë“¤
        error_patterns = [
            (r'ERROR', 'ERROR'),
            (r'CRITICAL', 'CRITICAL'),
            (r'Exception:', 'EXCEPTION'),
            (r'Traceback', 'TRACEBACK'),
            (r'Failed', 'FAILED'),
            (r'Error:', 'ERROR'),
        ]
        
        for pattern, error_type in error_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
                timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                timestamp = timestamp_match.group(1) if timestamp_match else 'Unknown'
                
                return {
                    'timestamp': timestamp,
                    'type': error_type,
                    'file': file_path.name,
                    'line_num': line_num,
                    'message': line.strip(),
                    'full_path': str(file_path)
                }
        
        return None
    
    def find_related_commits(self, error_date: str) -> List[str]:
        """ì˜¤ë¥˜ ìˆ˜ì • ê´€ë ¨ ì»¤ë°‹ ì°¾ê¸°"""
        try:
            # ì˜¤ë¥˜ ë‚ ì§œ ì´í›„ì˜ fix ì»¤ë°‹ ì°¾ê¸°
            result = subprocess.run(
                ['git', 'log', f'--since={error_date}', '--grep=fix', '--oneline'],
                capture_output=True, text=True, cwd=PROJECT_ROOT
            )
            if result.stdout:
                return result.stdout.strip().split('\n')[:5]
        except:
            pass
        return []
    
    def analyze_patterns(self, errors: List[Dict]) -> Dict[str, any]:
        """ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„"""
        analysis = {
            'total_errors': len(errors),
            'by_type': defaultdict(int),
            'by_file': defaultdict(int),
            'by_hour': defaultdict(int),
            'common_messages': defaultdict(int)
        }
        
        for error in errors:
            # íƒ€ì…ë³„ ì§‘ê³„
            analysis['by_type'][error['type']] += 1
            
            # íŒŒì¼ë³„ ì§‘ê³„
            analysis['by_file'][error['file']] += 1
            
            # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
            if error['timestamp'] != 'Unknown':
                try:
                    hour = datetime.strptime(error['timestamp'], '%Y-%m-%d %H:%M:%S').hour
                    analysis['by_hour'][hour] += 1
                except:
                    pass
            
            # ë©”ì‹œì§€ íŒ¨í„´
            # ìˆ«ì, UUID ë“±ì„ ì œê±°í•˜ì—¬ íŒ¨í„´í™”
            cleaned_msg = re.sub(r'\d+', 'N', error['message'])
            cleaned_msg = re.sub(r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}', 'UUID', cleaned_msg)
            analysis['common_messages'][cleaned_msg[:100]] += 1
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ë©”ì‹œì§€ë§Œ ìœ ì§€
        analysis['common_messages'] = dict(
            sorted(analysis['common_messages'].items(), 
                   key=lambda x: x[1], reverse=True)[:10]
        )
        
        return dict(analysis)
    
    def generate_report(self, output_file: Optional[str] = None):
        """ì˜¤ë¥˜ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ” ì˜¤ë¥˜ ì¶”ì  ì‹œì‘...")
        
        # ìµœê·¼ 7ì¼ ì˜¤ë¥˜ ìŠ¤ìº”
        errors = self.scan_log_files(days=7)
        print(f"âœ… {len(errors)}ê°œì˜ ì˜¤ë¥˜ ë°œê²¬")
        
        # íŒ¨í„´ ë¶„ì„
        analysis = self.analyze_patterns(errors)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'generated_at': datetime.now().isoformat(),
            'summary': {
                'total_errors': analysis['total_errors'],
                'unique_files': len(analysis['by_file']),
                'error_types': dict(analysis['by_type'])
            },
            'patterns': {
                'by_type': dict(analysis['by_type']),
                'by_file': dict(analysis['by_file']),
                'by_hour': dict(analysis['by_hour']),
                'common_messages': analysis['common_messages']
            },
            'recent_errors': errors[:20]  # ìµœê·¼ 20ê°œ
        }
        
        # ì˜¤ë¥˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self._update_error_history(errors, analysis)
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
        
        # ì½˜ì†” ì¶œë ¥
        self._print_summary(analysis)
        
        return report
    
    def _update_error_history(self, errors: List[Dict], analysis: Dict):
        """ERROR_HISTORY.md íŒŒì¼ ì—…ë°ì´íŠ¸"""
        history_file = CLAUDE_DIR / 'ERROR_HISTORY.md'
        
        if not history_file.exists():
            return
        
        # ìƒˆë¡œìš´ ì„¹ì…˜ ì¶”ê°€
        today = datetime.now().strftime('%Y-%m-%d')
        new_section = f"\n### {today}: ìë™ ìŠ¤ìº” ê²°ê³¼\n"
        new_section += f"- **ìŠ¤ìº”ëœ ì˜¤ë¥˜**: {len(errors)}ê°œ\n"
        new_section += f"- **ì£¼ìš” íƒ€ì…**: {', '.join(f'{k}({v})' for k, v in list(analysis['by_type'].items())[:3])}\n"
        new_section += "- **ìƒíƒœ**: ğŸ” ë¶„ì„ ì¤‘\n"
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ì˜¤ë¥˜ ì¶”ê°€
        if analysis['common_messages']:
            new_section += "- **ë¹ˆë²ˆí•œ ì˜¤ë¥˜**:\n"
            for msg, count in list(analysis['common_messages'].items())[:3]:
                new_section += f"  - {msg[:50]}... ({count}íšŒ)\n"
        
        # íŒŒì¼ì— ì¶”ê°€
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì ì ˆí•œ ìœ„ì¹˜ì— ì‚½ì…
            if '## 2025ë…„' in content:
                parts = content.split('## 2025ë…„', 1)
                content = parts[0] + '## 2025ë…„' + new_section + parts[1]
            else:
                content += f"\n## {datetime.now().year}ë…„\n" + new_section
            
            with open(history_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print("âœ… ERROR_HISTORY.md ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _print_summary(self, analysis: Dict):
        """ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ“Š ì˜¤ë¥˜ ë¶„ì„ ìš”ì•½")
        print("="*50)
        
        print(f"\nì´ ì˜¤ë¥˜ ìˆ˜: {analysis['total_errors']}")
        
        print("\nì˜¤ë¥˜ íƒ€ì…ë³„:")
        for error_type, count in sorted(analysis['by_type'].items(), key=lambda x: x[1], reverse=True):
            print(f"  - {error_type}: {count}ê°œ")
        
        print("\níŒŒì¼ë³„ ì˜¤ë¥˜:")
        for file_name, count in sorted(analysis['by_file'].items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  - {file_name}: {count}ê°œ")
        
        print("\nì‹œê°„ëŒ€ë³„ ë¶„í¬:")
        peak_hours = sorted(analysis['by_hour'].items(), key=lambda x: x[1], reverse=True)[:3]
        for hour, count in peak_hours:
            print(f"  - {hour:02d}ì‹œ: {count}ê°œ")
        
        print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        if analysis['total_errors'] > 100:
            print("  - ì˜¤ë¥˜ê°€ ë§ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì•ˆì •ì„± ì ê²€ í•„ìš”")
        if peak_hours and peak_hours[0][1] > analysis['total_errors'] * 0.3:
            print(f"  - {peak_hours[0][0]}ì‹œì— ì˜¤ë¥˜ ì§‘ì¤‘. í•´ë‹¹ ì‹œê°„ëŒ€ ë¶€í•˜ í™•ì¸ í•„ìš”")
        if 'CRITICAL' in analysis['by_type']:
            print("  - CRITICAL ì˜¤ë¥˜ ë°œê²¬. ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì˜¤ë¥˜ ì¶”ì  ë° ë¶„ì„')
    parser.add_argument('--days', type=int, default=7, help='ë¶„ì„í•  ì¼ìˆ˜')
    parser.add_argument('--output', type=str, help='ë¦¬í¬íŠ¸ ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--update-history', action='store_true', help='ERROR_HISTORY.md ì—…ë°ì´íŠ¸')
    
    args = parser.parse_args()
    
    tracker = ErrorTracker()
    report = tracker.generate_report(output_file=args.output)
    
    print("\nâœ… ì˜¤ë¥˜ ì¶”ì  ì™„ë£Œ!")

if __name__ == "__main__":
    main()